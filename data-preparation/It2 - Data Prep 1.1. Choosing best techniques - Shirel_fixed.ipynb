{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6679ab839dfc599",
   "metadata": {},
   "source": [
    "# Data Preparation - IT2 - Choosing |best techniques per step in the flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db9fa264159cd5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8bdef35d1e5da",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the path to the folder containing the images to be processed\n",
    "folder_path = '../data/subset'  # Update this path to point to your specific folder containing images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22a19fd22d2b9a",
   "metadata": {},
   "source": [
    "## Loading Images and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703777edb6b6cb6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder_path, extensions=('.png', '.jpg', '.jpeg', '.JPG')):\n",
    "    \"\"\"\n",
    "    Load all image file paths from a specified folder that match the given file extensions.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): The path to the folder containing the images.\n",
    "    extensions (tuple of str): A tuple of file extensions to filter the images by. \n",
    "                               Default is ('.png', '.jpg', '.jpeg', '.JPG').\n",
    "\n",
    "    Returns:\n",
    "    list: A list of full file paths to images in the folder that match the specified extensions.\n",
    "    \n",
    "    Raises:\n",
    "    FileNotFoundError: If the specified folder does not exist.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"The specified folder does not exist: {folder_path}\")\n",
    "\n",
    "    # List comprehension to gather all image paths with the specified extensions\n",
    "    image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(extensions)]\n",
    "\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097d12ed08ed95c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to convert to gray scale\n",
    "def load_and_preprocess_images(image_paths, resize_dim=(256, 256)):\n",
    "    images = []\n",
    "    image_ids = []\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    for path in tqdm(image_paths, desc=\"Loading and preprocessing images\", unit=\"image\"):\n",
    "        img = cv2.imread(path)\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "        # img_resized = cv2.resize(img_gray, resize_dim)  # Resize for consistency\n",
    "        images.append(img_gray)\n",
    "        image_ids.append(f'Image_{len(images)}')  # Assign image ID as Image_1, Image_2, etc.\n",
    "\n",
    "    return images, image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0024219894c26ef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the CSV file with the image statistics\n",
    "images_stats_path = \"../data-understanding/images_stats.csv\"  \n",
    "images_stats_df = pd.read_csv(images_stats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db335d32477016cb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_stats_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7addce2a50971",
   "metadata": {},
   "source": [
    "## Functions per step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7d2143725450b",
   "metadata": {},
   "source": [
    "### Step 1: Noise Reduction Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3ee9d85584a63",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Noise Reduction Functions\n",
    "def apply_gaussian_blur(image, ksize=(5, 5)):\n",
    "    \"\"\"Apply Gaussian Blur to reduce noise with the specified kernel size.\"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 0)\n",
    "\n",
    "def apply_median_blur(image, ksize=5):\n",
    "    \"\"\"Apply Median Blur to reduce salt-and-pepper noise with the specified kernel size.\"\"\"\n",
    "    return cv2.medianBlur(image, ksize)\n",
    "\n",
    "def apply_non_local_means(image, h=10, templateWindowSize=7, searchWindowSize=21):\n",
    "    \"\"\"Apply Non-Local Means Denoising with specified parameters.\"\"\"\n",
    "    return cv2.fastNlMeansDenoising(image, None, h, templateWindowSize, searchWindowSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cdd42d07d1c96",
   "metadata": {},
   "source": [
    "### Step 2: Histogram Equalization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c99f9c5258c6f3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram Equalization Functions\n",
    "def apply_histogram_equalization(image):\n",
    "    return cv2.equalizeHist(image)\n",
    "\n",
    "def apply_clahe(image, clipLimit=2.0, tileGridSize=(8, 8)):\n",
    "    \"\"\"Apply CLAHE to enhance image contrast with specified parameters.\"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n",
    "    return clahe.apply(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e647d0a546cb82",
   "metadata": {},
   "source": [
    "### Step 3: Binarization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2691be436fc225",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Binarization Functions\n",
    "def apply_global_threshold(image, thresholdValue=127):\n",
    "    \"\"\"Apply Global Thresholding with the specified threshold value.\"\"\"\n",
    "    _, binary_image = cv2.threshold(image, thresholdValue, 255, cv2.THRESH_BINARY)\n",
    "    return binary_image\n",
    "\n",
    "def apply_adaptive_threshold(image, adaptiveMethod=cv2.ADAPTIVE_THRESH_MEAN_C, blockSize=11, C=2):\n",
    "    \"\"\"Apply Adaptive Thresholding with the specified method, block size, and constant C.\"\"\"\n",
    "    return cv2.adaptiveThreshold(image, 255, adaptiveMethod, cv2.THRESH_BINARY, blockSize, C)\n",
    "\n",
    "def apply_otsu_threshold(image):\n",
    "    \"\"\"Apply Otsu Thresholding.\"\"\"\n",
    "    _, binary_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return binary_image\n",
    "\n",
    "def apply_inverted_otsu_threshold(image):\n",
    "    \"\"\"Apply Inverted Otsu Thresholding.\"\"\"\n",
    "    _, binary_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    return binary_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7a336f8c8b717",
   "metadata": {},
   "source": [
    "### Step 4: Morphological Operations Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08a64db428ad4a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Morphological Operations Functions\n",
    "def apply_dilation(image, kernel_size=(5, 5)):\n",
    "    \"\"\"Apply Dilation with the specified kernel size.\"\"\"\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    return cv2.dilate(image, kernel, iterations=1)\n",
    "\n",
    "def apply_erosion(image, kernel_size=(5, 5)):\n",
    "    \"\"\"Apply Erosion with the specified kernel size.\"\"\"\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    return cv2.erode(image, kernel, iterations=1)\n",
    "\n",
    "def apply_opening(image, kernel_size=(5, 5)):\n",
    "    \"\"\"Apply Morphological Opening with the specified kernel size.\"\"\"\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "def apply_closing(image, kernel_size=(5, 5)):\n",
    "    \"\"\"Apply Morphological Closing with the specified kernel size.\"\"\"\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    return cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5596f364429dda1",
   "metadata": {},
   "source": [
    "### Step 5: Edge Detection Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5c83f8a5c9bc8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Edge Detection Functions\n",
    "def apply_canny_edge(image, threshold1=50, threshold2=150):\n",
    "    \"\"\"Apply Canny Edge Detection with specified thresholds.\"\"\"\n",
    "    return cv2.Canny(image, threshold1, threshold2)\n",
    "\n",
    "def apply_sobel_edge(image, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT):\n",
    "    \"\"\"Apply Sobel Edge Detection with specified parameters.\"\"\"\n",
    "    return cv2.Sobel(image, cv2.CV_64F, 1, 1, ksize=ksize, scale=scale, delta=delta, borderType=borderType)\n",
    "\n",
    "def apply_unsharp_masking(image, amount=1.5, kernel_size=(0, 0)):\n",
    "    \"\"\"Apply Unsharp Masking to sharpen the image.\"\"\"\n",
    "    blurred = cv2.GaussianBlur(image, kernel_size, 0)\n",
    "    sharpened = cv2.addWeighted(image, 1 + amount, blurred, -amount, 0)\n",
    "    return sharpened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e5c4a578b19f5",
   "metadata": {},
   "source": [
    "## Characteristics Calculation for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec243807f00da68",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Image Characteristics Calculation Functions - from data understanding it2\n",
    "def calculate_brightness(image):\n",
    "    return np.mean(image)\n",
    "\n",
    "def calculate_sharpness(image):\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def calculate_contrast(image):\n",
    "    return image.std()\n",
    "\n",
    "def calculate_noise(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "    noise = cv2.absdiff(image, blurred)\n",
    "    return np.var(noise)\n",
    "\n",
    "# def calculate_skew(image):\n",
    "#     if len(image.shape) != 2:\n",
    "#         raise ValueError(\"Invalid image format. Image must be a 2D grayscale image.\")\n",
    "#     _, binary = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "#     coords = np.column_stack(np.where(binary > 0))\n",
    "#     if coords.size == 0:\n",
    "#         return 0\n",
    "#     angle = cv2.minAreaRect(coords)[-1]\n",
    "#     if angle < -45:\n",
    "#         angle = -(90 + angle)\n",
    "#     else:\n",
    "#         angle = -angle\n",
    "#     if abs(angle) < 1e-2:\n",
    "#         angle = 0\n",
    "#     return round(angle, 2)\n",
    "# \n",
    "# def calculate_line_spacing(image):\n",
    "#     if len(image.shape) != 2:\n",
    "#         raise ValueError(\"Invalid image format. Image must be a 2D grayscale image.\")\n",
    "#     _, binary = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "#     contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     heights = [cv2.boundingRect(contour)[3] for contour in contours]\n",
    "#     if len(heights) > 1:\n",
    "#         line_spacing = np.mean(np.diff(sorted(heights)))\n",
    "#     else:\n",
    "#         line_spacing = 0\n",
    "#     return line_spacing\n",
    "# \n",
    "# def detect_tables(image):\n",
    "#     if len(image.shape) != 2:\n",
    "#         raise ValueError(\"Invalid image format. Image must be a 2D grayscale image.\")\n",
    "#     _, binary = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "#     binary = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "#     contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     table_contours = [contour for contour in contours if cv2.contourArea(contour) > 1000]\n",
    "#     return len(table_contours)\n",
    "# \n",
    "# def calculate_resolution(image):\n",
    "#     height, width = image.shape[:2]\n",
    "#     return height * width\n",
    "# \n",
    "# def calculate_elements_detection(image):\n",
    "#     if len(image.shape) != 2:\n",
    "#         raise ValueError(\"Invalid image format. Image must be a 2D grayscale image.\")\n",
    "#     _, binary = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "#     contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     return len(contours)\n",
    "# \n",
    "# def calculate_texture(image):\n",
    "#     laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "#     return laplacian.std()\n",
    "# \n",
    "# def calculate_patterns(image):\n",
    "#     if len(image.shape) != 2:\n",
    "#         raise ValueError(\"Invalid image format. Image must be a 2D grayscale image.\")\n",
    "#     edges = cv2.Canny(image, 100, 200)\n",
    "#     return np.sum(edges > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203cff1e1647315b",
   "metadata": {},
   "source": [
    "## Evaluation per step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dafdc80cafbb5f",
   "metadata": {},
   "source": [
    "### Function of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72107041cb13b3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Evaluation Function\n",
    "# -------------------------\n",
    "# def basic_evaluation(image, techniques_dict, original_stats):\n",
    "#     evaluation_results = {}\n",
    "#     for technique_name, technique_func in techniques_dict.items():\n",
    "#         processed_image = technique_func(image)\n",
    "#         stats = {\n",
    "#             \"Brightness\": calculate_brightness(processed_image),\n",
    "#             \"Sharpness\": calculate_sharpness(processed_image),\n",
    "#             \"Contrast\": calculate_contrast(processed_image),\n",
    "#             \"Noise\": calculate_noise(processed_image)\n",
    "#         }\n",
    "# \n",
    "#         # Basic scoring function - prioritizing sharpness, contrast, and minimized noise\n",
    "#         score = stats[\"Sharpness\"] + stats[\"Contrast\"] - stats[\"Noise\"]\n",
    "#         evaluation_results[technique_name] = {\"Score\": score, \"Stats\": stats}\n",
    "# \n",
    "#     best_technique = max(evaluation_results, key=lambda x: evaluation_results[x][\"Score\"])\n",
    "#     return {\"Best Technique\": best_technique, \"Evaluation Results\": evaluation_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067606c27e5a0cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def advanced_evaluation(image, techniques_dict, original_stats):\n",
    "#     evaluation_results = {}\n",
    "# \n",
    "#     for technique_name, technique_func in techniques_dict.items():\n",
    "#         # Apply the technique\n",
    "#         processed_image = technique_func(image)\n",
    "# \n",
    "#         # Calculate characteristics for the processed image\n",
    "#         stats = {\n",
    "#             \"Brightness\": calculate_brightness(processed_image),\n",
    "#             \"Sharpness\": calculate_sharpness(processed_image),\n",
    "#             \"Contrast\": calculate_contrast(processed_image),\n",
    "#             \"Noise\": calculate_noise(processed_image),\n",
    "#             \"Skew\": calculate_skew(processed_image),\n",
    "#             \"Line Spacing\": calculate_line_spacing(processed_image),\n",
    "#             \"Tables Detected\": detect_tables(processed_image),\n",
    "#             \"Resolution\": calculate_resolution(processed_image),\n",
    "#             \"Detected Elements\": calculate_elements_detection(processed_image),\n",
    "#             \"Texture\": calculate_texture(processed_image),\n",
    "#             \"Patterns\": calculate_patterns(processed_image)\n",
    "#         }\n",
    "# \n",
    "#         # Normalize metrics to comparable ranges (between 0 and 1, roughly)\n",
    "#         stats_normalized = {\n",
    "#             \"Brightness\": stats[\"Brightness\"] / 255,\n",
    "#             \"Sharpness\": stats[\"Sharpness\"] / 1000,\n",
    "#             \"Contrast\": stats[\"Contrast\"] / 255,\n",
    "#             \"Noise\": stats[\"Noise\"] / 255,\n",
    "#             \"Skew\": stats[\"Skew\"] / 45,\n",
    "#             \"Line Spacing\": stats[\"Line Spacing\"] / 100,\n",
    "#             \"Tables Detected\": stats[\"Tables Detected\"] / 10,\n",
    "#             \"Resolution\": stats[\"Resolution\"] / (512 * 512),\n",
    "#             \"Detected Elements\": stats[\"Detected Elements\"] / 100,\n",
    "#             \"Texture\": stats[\"Texture\"] / 100,\n",
    "#             \"Patterns\": stats[\"Patterns\"] / 1000\n",
    "#         }\n",
    "# \n",
    "#         # Normalize the original stats for comparison\n",
    "#         original_stats_normalized = {\n",
    "#             \"Brightness\": original_stats[\"Brightness\"] / 255,\n",
    "#             \"Sharpness\": original_stats[\"Sharpness\"] / 1000,\n",
    "#             \"Contrast\": original_stats[\"Contrast\"] / 255,\n",
    "#             \"Noise\": original_stats[\"Noise\"] / 255,\n",
    "#             \"Skew\": original_stats[\"Skew\"] / 45,\n",
    "#             \"Line Spacing\": original_stats[\"Line Spacing\"] / 100,\n",
    "#             \"Tables Detected\": original_stats[\"Tables Detected\"] / 10,\n",
    "#             \"Resolution\": original_stats[\"Resolution\"] / (512 * 512),\n",
    "#             \"Detected Elements\": original_stats[\"Detected Elements\"] / 100,\n",
    "#             \"Texture\": original_stats[\"Texture\"] / 100,\n",
    "#             \"Patterns\": original_stats[\"Patterns\"] / 1000\n",
    "#         }\n",
    "# \n",
    "#         # Weights for each characteristic (to determine their importance)\n",
    "#         weights = {\n",
    "#             \"Brightness\": -1.0,  # Closer to original is better (penalized if different)\n",
    "#             \"Sharpness\": 2.0,    # Higher is better (rewarded if improved)\n",
    "#             \"Contrast\": 1.0,     # Higher is better (rewarded if improved)\n",
    "#             \"Noise\": -1.5,       # Lower is better (penalized if increased)\n",
    "#             \"Skew\": -0.5,        # Closer to original is better (penalized if different)\n",
    "#             \"Line Spacing\": -0.5,  # Closer to original is better (penalized if different)\n",
    "#             \"Tables Detected\": 1.0,  # More tables detected is better\n",
    "#             \"Resolution\": 1.0,    # Higher is better\n",
    "#             \"Detected Elements\": 1.0,  # More elements detected is better\n",
    "#             \"Texture\": 1.0,       # Higher texture complexity is better\n",
    "#             \"Patterns\": 1.0       # More patterns detected is better\n",
    "#         }\n",
    "# \n",
    "#         # Calculate score using normalized metrics and weights\n",
    "#         score = 0\n",
    "#         for metric, value in stats_normalized.items():\n",
    "#             original_value = original_stats_normalized.get(metric, 0)\n",
    "#             score += weights[metric] * (value - original_value)\n",
    "# \n",
    "#         evaluation_results[technique_name] = {\"Score\": score, \"Stats\": stats}\n",
    "# \n",
    "#     # Determine the best technique based on the highest score\n",
    "#     best_technique = max(evaluation_results, key=lambda x: evaluation_results[x][\"Score\"])\n",
    "#     return {\"Best Technique\": best_technique, \"Evaluation Results\": evaluation_results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14621ade3e0e3283",
   "metadata": {},
   "source": [
    "##### Normalization Process\n",
    "\n",
    "**Normalization** is crucial for ensuring that the values of different characteristics (`Brightness`, `Sharpness`, `Contrast`, `Noise`) are on a similar scale. Without normalization, these characteristics might have vastly different ranges, which could skew the evaluation. Here's what happens in the function:\n",
    "\n",
    "1. **Brightness Normalization**:\n",
    "   - The brightness of an image is typically represented on a scale from 0 to 255 (as an 8-bit grayscale value).\n",
    "   - To normalize brightness, we divide it by 255, which brings its range between 0 and 1.\n",
    "\n",
    "2. **Sharpness Normalization**:\n",
    "   - Sharpness is measured as the variance of the Laplacian, which often has larger values than brightness.\n",
    "   - Dividing by `1000` helps normalize it to roughly between 0 and 1. The choice of `1000` is made to ensure that sharpness values are comparable to the other metrics.\n",
    "\n",
    "3. **Contrast Normalization**:\n",
    "   - Contrast is calculated using the standard deviation of pixel values, which usually falls between 0 and 255 for 8-bit images.\n",
    "   - Dividing by 255 brings contrast into the range between 0 and 1.\n",
    "\n",
    "4. **Noise Normalization**:\n",
    "   - The noise measure is the variance of the difference between the original and a blurred version of the image.\n",
    "   - Dividing by 255 brings it to a similar range as the other characteristics, ensuring comparability.\n",
    "\n",
    "By normalizing all metrics to a range between 0 and 1, we ensure that each characteristic has equal weight in the evaluation, preventing one metric from dominating due to a larger numeric range.\n",
    "\n",
    "##### Scoring Calculation\n",
    "\n",
    "Once all metrics are normalized, a score is calculated to determine how well the processed image improves compared to the original. Here's the breakdown of the scoring process:\n",
    "\n",
    "1. **Weights for Characteristics**:\n",
    "   - We assign **weights** to each metric based on its importance:\n",
    "     - **Brightness**: Weight of `-1.0` means that deviation from the original value is penalized.\n",
    "     - **Sharpness**: Weight of `2.0` rewards increased sharpness.\n",
    "     - **Contrast**: Weight of `1.0` rewards increased contrast.\n",
    "     - **Noise**: Weight of `-1.5` penalizes increased noise.\n",
    "\n",
    "2. **Score Calculation**:\n",
    "   - The difference between the normalized processed value and the normalized original value is multiplied by the respective weight.\n",
    "   - If a **positively weighted metric** (like sharpness or contrast) **improves**, it contributes positively to the score.\n",
    "   - If a **negatively weighted metric** (like noise or brightness deviation) **increases**, it contributes negatively, penalizing the score.\n",
    "\n",
    "3. **Best Technique Selection**:\n",
    "   - After calculating the score for each technique, the function selects the one with the **highest score**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4a8b130aa7d7f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def advanced_evaluation(image, techniques_dict, original_stats):\n",
    "    evaluation_results = {}\n",
    "\n",
    "    for technique_name, technique_func in techniques_dict.items():\n",
    "        # Apply the technique\n",
    "        processed_image = technique_func(image)\n",
    "\n",
    "        # Calculate characteristics for the processed image\n",
    "        stats = {\n",
    "            \"Brightness\": calculate_brightness(processed_image),\n",
    "            \"Sharpness\": calculate_sharpness(processed_image),\n",
    "            \"Contrast\": calculate_contrast(processed_image),\n",
    "            \"Noise\": calculate_noise(processed_image),\n",
    "        }\n",
    "\n",
    "        # Normalize metrics to comparable ranges (between 0 and 1, roughly)\n",
    "        stats_normalized = {\n",
    "            \"Brightness\": stats[\"Brightness\"] / 255,\n",
    "            \"Sharpness\": stats[\"Sharpness\"] / 1000,\n",
    "            \"Contrast\": stats[\"Contrast\"] / 255,\n",
    "            \"Noise\": stats[\"Noise\"] / 255,\n",
    "        }\n",
    "\n",
    "        # Normalize the original stats for comparison\n",
    "        original_stats_normalized = {\n",
    "            \"Brightness\": original_stats[\"Brightness\"] / 255,\n",
    "            \"Sharpness\": original_stats[\"Sharpness\"] / 1000,\n",
    "            \"Contrast\": original_stats[\"Contrast\"] / 255,\n",
    "            \"Noise\": original_stats[\"Noise\"] / 255,\n",
    "        }\n",
    "\n",
    "        # Weights for each characteristic (to determine their importance)\n",
    "        weights = {\n",
    "            \"Brightness\": 1.0,  # Higher is better (rewarded if improved)\n",
    "            \"Sharpness\": 1.0,    # Higher is better (rewarded if improved) but images were generally sharp already \n",
    "            \"Contrast\": 2.0,     # Higher is better (rewarded if improved) the levels of contrast were lower and obstructed details\n",
    "            \"Noise\": -1.5,       # Lower is better (penalized if increased)\n",
    "        }\n",
    "\n",
    "        # Calculate score using normalized metrics and weights\n",
    "        score = 0\n",
    "        for metric, value in stats_normalized.items():\n",
    "            original_value = original_stats_normalized.get(metric, 0)\n",
    "            score += weights[metric] * (value - original_value)\n",
    "\n",
    "        evaluation_results[technique_name] = {\"Score\": score, \"Stats\": stats}\n",
    "\n",
    "    # Determine the best technique based on the highest score\n",
    "    best_technique = max(evaluation_results, key=lambda x: evaluation_results[x][\"Score\"])\n",
    "    return {\"Best Technique\": best_technique, \"Evaluation Results\": evaluation_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dcfaa1991f5e3a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function for Each Step testing\n",
    "def run_step(step_name, techniques_dict, test_images, test_image_ids, best_techniques_list):\n",
    "    print(f\"\\nRunning Step: {step_name}\\n{'-' * 40}\")\n",
    "    all_results = []\n",
    "\n",
    "    # Folder to save images from this step\n",
    "    output_folder_step = f\"./Data/It2/{step_name}\"\n",
    "    os.makedirs(output_folder_step, exist_ok=True)\n",
    "\n",
    "    # Image ID to be saved for comparison (using the first image ID consistently for all techniques)\n",
    "    save_image_id = test_image_ids[0] if len(test_image_ids) > 0 else None\n",
    "\n",
    "    for img, img_id in zip(test_images, test_image_ids):\n",
    "        # Retrieve original stats from the dataset for the specific image being processed\n",
    "        original_stats = images_stats_df[images_stats_df['Image'] == img_id].iloc[0].to_dict()\n",
    "\n",
    "        # Evaluate each technique on the current image\n",
    "        step_result = advanced_evaluation(img, techniques_dict, original_stats)\n",
    "        all_results.append((img_id, original_stats, step_result))\n",
    "        print(f\"Best Technique for {img_id}: {step_result['Best Technique']}\")\n",
    "\n",
    "        # Save one processed image per technique per step (consistent image ID across all techniques)\n",
    "        if img_id == save_image_id:\n",
    "            for technique_name, technique_func in techniques_dict.items():\n",
    "                # Apply the technique to the image\n",
    "                processed_image = technique_func(img)\n",
    "\n",
    "                # Save the processed image\n",
    "                image_save_path = f\"{output_folder_step}/{technique_name}_Image_{img_id}.jpg\"\n",
    "                cv2.imwrite(image_save_path, processed_image)\n",
    "\n",
    "    # Generate Comparison Table\n",
    "    comparison_data = []\n",
    "    for img_id, original_stats, result in all_results:\n",
    "        # Add original stats row\n",
    "        comparison_data.append([img_id, \"Original\"] + list(original_stats.values())[1:])  # Skip the 'Image' key\n",
    "        # Add each technique's stats\n",
    "        for technique, metrics in result[\"Evaluation Results\"].items():\n",
    "            comparison_data.append([img_id, f\"{step_name} - {technique}\"] + list(metrics[\"Stats\"].values()))\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    comparison_df = pd.DataFrame(comparison_data, columns=[\n",
    "        \"Image_ID\", \"Technique\", \"Brightness\", \"Sharpness\", \"Contrast\", \"Noise\"\n",
    "    ])\n",
    "\n",
    "    # Generate Recommendation\n",
    "    recommended_technique_name = max(all_results, key=lambda x: x[2][\"Evaluation Results\"][x[2][\"Best Technique\"]][\"Score\"])[2][\"Best Technique\"]\n",
    "    recommended_technique_func = techniques_dict[recommended_technique_name]\n",
    "    print(f\"\\nRecommended Technique for {step_name}: {recommended_technique_name}\\n\")\n",
    "\n",
    "    # Append both technique name and function for further tuning\n",
    "    best_techniques_list.append((step_name, recommended_technique_name, recommended_technique_func))\n",
    "\n",
    "    # Return the comparison DataFrame\n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473417fb6fc8aff",
   "metadata": {},
   "source": [
    "### Running Different Techniques per Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b64a356573a62",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load all image file paths from the specified folder\n",
    "image_paths_all = load_images_from_folder(folder_path)\n",
    "\n",
    "# Load and preprocess all images\n",
    "total_images, total_image_ids = load_and_preprocess_images(image_paths_all)\n",
    "\n",
    "# Randomly select 5 images for experimentation\n",
    "experiment_indices = random.sample(range(len(total_images)), 5)\n",
    "test_images = [total_images[i] for i in experiment_indices]\n",
    "test_image_ids = [total_image_ids[i] for i in experiment_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d321de124a8f61b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_techniques_list = []\n",
    "comparison_tables = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272df62648886a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Noise Reduction\n",
    "noise_reduction_techniques = {\n",
    "    \"Gaussian Blur\": lambda img: cv2.GaussianBlur(img, (5, 5), 0),\n",
    "    \"Median Blur\": lambda img: cv2.medianBlur(img, 5),\n",
    "    \"Non-Local Means\": lambda img: cv2.fastNlMeansDenoising(img, None, 10, 7, 21)\n",
    "}\n",
    "comparison_tables.append(run_step(\"Noise Reduction\", noise_reduction_techniques, test_images, test_image_ids, best_techniques_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef0203d606d09a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: Histogram Equalization\n",
    "histogram_equalization_techniques = {\n",
    "    \"Histogram Equalization\": lambda img: cv2.equalizeHist(img),\n",
    "    \"CLAHE\": lambda img: cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(img)\n",
    "}\n",
    "comparison_tables.append(run_step(\"Histogram Equalization\", histogram_equalization_techniques, test_images, test_image_ids, best_techniques_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68616ef9a33f67d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3: Binarization\n",
    "binarization_techniques = {\n",
    "    \"Global Threshold\": lambda img: cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)[1],\n",
    "    \"Adaptive Threshold\": lambda img: cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2),\n",
    "    \"Otsu Threshold\": lambda img: cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1],\n",
    "    \"Inverted Otsu Threshold\": lambda img: cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "}\n",
    "comparison_tables.append(run_step(\"Binarization\", binarization_techniques, test_images, test_image_ids, best_techniques_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8bf5b17df76bee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 4: Morphological Operations\n",
    "morphological_operations_techniques = {\n",
    "    \"Dilation\": lambda img: cv2.dilate(img, np.ones((5, 5), np.uint8), iterations=1),\n",
    "    \"Erosion\": lambda img: cv2.erode(img, np.ones((5, 5), np.uint8), iterations=1),\n",
    "    \"Opening\": lambda img: cv2.morphologyEx(img, cv2.MORPH_OPEN, np.ones((5, 5), np.uint8)),\n",
    "    \"Closing\": lambda img: cv2.morphologyEx(img, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
    "}\n",
    "comparison_tables.append(run_step(\"Morphological Operations\", morphological_operations_techniques, test_images, test_image_ids, best_techniques_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47a9227209a883",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 5: Edge Detection\n",
    "edge_detection_techniques = {\n",
    "    \"Canny Edge\": lambda img: cv2.Canny(img, 100, 200),\n",
    "    \"Sobel Edge\": lambda img: cv2.convertScaleAbs(cv2.Sobel(img, cv2.CV_64F, 1, 1, ksize=3)),\n",
    "    \"Unsharp Mask\": lambda img: cv2.addWeighted(img, 1.5, cv2.GaussianBlur(img, (0, 0), 3), -0.5, 0)\n",
    "}\n",
    "comparison_tables.append(run_step(\"Edge Detection\", edge_detection_techniques, test_images, test_image_ids, best_techniques_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439513ab9cfdf06",
   "metadata": {},
   "source": [
    "## Final best techniques per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264edc32f3787a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the list of best techniques for each step\n",
    "print(\"\\nBest Techniques for Each Step:\")\n",
    "for step, technique_name, technique_func in best_techniques_list:\n",
    "    print(f\"{step}: {technique_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b12bb0b5a2f14c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, comparison_df in enumerate(comparison_tables):\n",
    "    comparison_df.to_csv(f\"comparison tables/comparison_table_step_{i+1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cfb8654a8557fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display comparison tables within the notebook\n",
    "for i, comparison_df in enumerate(comparison_tables):\n",
    "    print(f\"Comparison Table for Step {i+1}:\")\n",
    "    display(comparison_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d20d42c755cc5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Average Comparison Table\n",
    "average_comparison_data = []\n",
    "for comparison_df in comparison_tables:\n",
    "    avg_stats = comparison_df.groupby(\"Technique\").mean().reset_index()\n",
    "    average_comparison_data.append(avg_stats)\n",
    "\n",
    "# Combine average stats from all steps\n",
    "average_comparison_df = pd.concat(average_comparison_data, ignore_index=True)\n",
    "# Save the average comparison table to a CSV file\n",
    "average_comparison_df.to_csv(\"comparison tables/average_comparison_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7133144103efbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d61986dc386f3b",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8595b15fd7fa24",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning Function\n",
    "def hyperparameter_tuning(images, best_techniques_list, param_grids, evaluation_function):\n",
    "    tuned_results = {}\n",
    "\n",
    "    # Choose one image to save for comparison purposes\n",
    "    save_image_id = total_image_ids[0] if len(total_image_ids) > 0 else None\n",
    "\n",
    "    for step_name, technique_name, best_technique_func in best_techniques_list:\n",
    "        print(f\"\\nHyperparameter Tuning for Step: {step_name}\\n{'-' * 40}\")\n",
    "        best_params = None\n",
    "        best_score = -np.inf\n",
    "        param_grid = param_grids.get(technique_name, [])\n",
    "\n",
    "        for params in param_grid:\n",
    "            total_score = 0\n",
    "\n",
    "            # Convert parameter dict to string for filename (e.g., 'param1_val1_param2_val2')\n",
    "            params_str = \"_\".join([f\"{key}_{value}\" for key, value in params.items()])\n",
    "\n",
    "            for img, img_id in zip(images, total_image_ids):\n",
    "                try:\n",
    "                    # Apply the best technique with the given parameters explicitly based on technique name\n",
    "                    if technique_name == \"Gaussian Blur\":\n",
    "                        processed_image = apply_gaussian_blur(img, **params)\n",
    "                    elif technique_name == \"Median Blur\":\n",
    "                        processed_image = apply_median_blur(img, **params)\n",
    "                    elif technique_name == \"Non-Local Means\":\n",
    "                        processed_image = apply_non_local_means(img, **params)\n",
    "                    elif technique_name == \"CLAHE\":\n",
    "                        processed_image = apply_clahe(img, **params)\n",
    "                    elif technique_name == \"Global Threshold\":\n",
    "                        processed_image = apply_global_threshold(img, **params)\n",
    "                    elif technique_name == \"Adaptive Threshold\":\n",
    "                        processed_image = apply_adaptive_threshold(img, **params)\n",
    "                    elif technique_name == \"Otsu Threshold\":\n",
    "                        processed_image = apply_otsu_threshold(img)\n",
    "                    elif technique_name == \"Inverted Otsu Threshold\":\n",
    "                        processed_image = apply_inverted_otsu_threshold(img)\n",
    "                    elif technique_name == \"Dilation\":\n",
    "                        processed_image = apply_dilation(img, **params)\n",
    "                    elif technique_name == \"Erosion\":\n",
    "                        processed_image = apply_erosion(img, **params)\n",
    "                    elif technique_name == \"Morphological Opening\":\n",
    "                        processed_image = apply_opening(img, **params)\n",
    "                    elif technique_name == \"Morphological Closing\":\n",
    "                        processed_image = apply_closing(img, **params)\n",
    "                    elif technique_name == \"Canny Edge\":\n",
    "                        processed_image = apply_canny_edge(img, **params)\n",
    "                    elif technique_name == \"Sobel Edge\":\n",
    "                        processed_image = apply_sobel_edge(img, **params)\n",
    "                    elif technique_name == \"Unsharp Masking\":\n",
    "                        processed_image = apply_unsharp_masking(img, **params)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown technique: {technique_name}\")\n",
    "\n",
    "                except TypeError as e:\n",
    "                    print(f\"Skipping parameters {params} due to TypeError: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Retrieve original stats for comparison\n",
    "                original_stats = images_stats_df[images_stats_df['Image'] == img_id].iloc[0].to_dict()\n",
    "                evaluation_result = evaluation_function(processed_image, {technique_name: best_technique_func}, original_stats)\n",
    "                step_score = evaluation_result[\"Evaluation Results\"][technique_name][\"Score\"]\n",
    "                total_score += step_score\n",
    "\n",
    "                # Save the processed image if it is the one designated for saving\n",
    "                if img_id == save_image_id:\n",
    "                    # Specify folder for best techniques and their parameters\n",
    "                    output_folder_tuning = f\"./Data/It2/Best Techniques/{step_name}_{technique_name}\"\n",
    "                    os.makedirs(output_folder_tuning, exist_ok=True)\n",
    "\n",
    "                    # Save the processed image\n",
    "                    cv2.imwrite(f\"{output_folder_tuning}/{technique_name}_Params_{params_str}_Image_{img_id}.jpg\", processed_image)\n",
    "\n",
    "            avg_score = total_score / len(images) if len(images) > 0 else -np.inf\n",
    "\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = params\n",
    "\n",
    "            print(f\"Parameters: {params}, Score: {avg_score}\")\n",
    "\n",
    "        tuned_results[step_name] = {\n",
    "            \"Best Parameters\": best_params,\n",
    "            \"Best Score\": best_score\n",
    "        }\n",
    "        print(f\"Best Parameters for {step_name}: {best_params} with Score: {best_score}\\n\")\n",
    "\n",
    "    return tuned_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d2b04c1e9428ca",
   "metadata": {},
   "source": [
    "#### Explanation of Techniques and Parameters\n",
    "\n",
    "##### 1. Noise Reduction\n",
    "###### Gaussian Blur (`ksize`)\n",
    "- **Parameter**: `ksize` (kernel size)\n",
    "- **Meaning**: Defines the extent of smoothing applied. A small kernel (e.g., `(3, 3)`) produces minimal blurring, preserving details, while larger kernels (e.g., `(9, 9)`) apply more significant blurring, which is useful for reducing noise but may remove finer details.\n",
    "- **Range**:\n",
    "  - `(3, 3)`, `(5, 5)`, `(7, 7)`, `(9, 9)`\n",
    "  - Smaller sizes preserve more details, larger sizes reduce noise more aggressively.\n",
    "\n",
    "###### Non-Local Means (`h`, `templateWindowSize`, `searchWindowSize`)\n",
    "- **Parameters**:\n",
    "  - `h`: Filtering strength (higher values = stronger filtering).\n",
    "  - `templateWindowSize`: Size of the patch used for comparison.\n",
    "  - `searchWindowSize`: Size of the window around the pixel for searching similar patches.\n",
    "- **Range**:\n",
    "  - `h`: `5`, `10`, `15`, `20`\n",
    "  - `templateWindowSize`: `7`, `10`\n",
    "  - `searchWindowSize`: `21`, `31`\n",
    "  - Balances noise reduction quality and processing time.\n",
    "\n",
    "###### Median Blur (`ksize`)\n",
    "- **Parameter**: `ksize` (kernel size)\n",
    "- **Meaning**: Reduces \"salt-and-pepper\" noise by replacing each pixel with the median of neighboring pixels. Larger kernels apply stronger noise reduction, potentially losing details.\n",
    "- **Range**:\n",
    "  - `3`, `5`, `7`, `9`\n",
    "  - Smaller values (`3`, `5`) are useful for mild noise; larger values (`7`, `9`) are effective for more significant noise.\n",
    "\n",
    "##### 2. Histogram Equalization\n",
    "###### CLAHE (`clipLimit`, `tileGridSize`)\n",
    "- **Parameters**:\n",
    "  - `clipLimit`: Controls contrast enhancement limit.\n",
    "  - `tileGridSize`: Size of the grid for local histogram equalization.\n",
    "- **Range**:\n",
    "  - `clipLimit`: `2.0` to `6.0`\n",
    "  - `tileGridSize`: `(4, 4)`, `(6, 6)`, `(8, 8)`\n",
    "  - Lower `clipLimit` values reduce noise amplification, larger `tileGridSize` produces smoother results.\n",
    "\n",
    "##### 3. Binarization\n",
    "###### Global Threshold (`thresholdValue`)\n",
    "- **Parameter**: `thresholdValue`\n",
    "- **Meaning**: Used to convert grayscale images to binary by comparing pixel values to a threshold. Lower values produce more white areas.\n",
    "- **Range**:\n",
    "  - `100`, `127`, `150`, `200`\n",
    "  - Balances the separation between foreground and background.\n",
    "\n",
    "###### Adaptive Threshold (`adaptiveMethod`, `blockSize`, `C`)\n",
    "- **Parameters**:\n",
    "  - `adaptiveMethod`: The method used for calculating the threshold (`cv2.ADAPTIVE_THRESH_MEAN_C` or `cv2.ADAPTIVE_THRESH_GAUSSIAN_C`).\n",
    "  - `blockSize`: Size of the local area considered for thresholding.\n",
    "  - `C`: Constant subtracted from the mean or weighted sum.\n",
    "- **Range**:\n",
    "  - `adaptiveMethod`: `cv2.ADAPTIVE_THRESH_MEAN_C` or `cv2.ADAPTIVE_THRESH_GAUSSIAN_C`\n",
    "  - `blockSize`: `11`, `15`\n",
    "  - `C`: `2`, `3`\n",
    "  - Allows adjustment to local image variations for better segmentation.\n",
    "\n",
    "###### Otsu Threshold\n",
    "- **Meaning**: Automatically determines the optimal threshold value to convert grayscale images to binary.\n",
    "- **Use**: Effective for images with bimodal histograms, making it suitable for foreground and background separation.\n",
    "- **Parameters**: None, Otsu's method calculates the optimal threshold automatically.\n",
    "\n",
    "###### Inverted Otsu Threshold\n",
    "- **Meaning**: Applies Otsu's method for thresholding but inverts the resulting binary image, making foreground black and background white.\n",
    "- **Use**: Useful when the target regions are originally white on a dark background.\n",
    "- **Parameters**: None, as Otsu's method calculates the optimal threshold automatically.\n",
    "\n",
    "##### 4. Morphological Operations\n",
    "###### Operation (`MORPH_OPEN`, `MORPH_CLOSE`, `DILATE`, `ERODE`, `kernel_size`)\n",
    "- **Parameters**:\n",
    "  - `operation`: The morphological transformation to apply.\n",
    "    - `cv2.MORPH_OPEN`: Removes small white noise.\n",
    "    - `cv2.MORPH_CLOSE`: Fills small black gaps in white areas.\n",
    "    - `cv2.MORPH_DILATE`: Expands white areas to connect small features.\n",
    "    - `cv2.MORPH_ERODE`: Shrinks white areas to reduce noise.\n",
    "  - `kernel_size`: Size of the structuring element.\n",
    "- **Range**:\n",
    "  - `kernel_size`: `(3, 3)`, `(5, 5)`, `(7, 7)`, `(9, 9)`\n",
    "  - Larger kernels apply more aggressive changes for connecting, removing, or shrinking features.\n",
    "\n",
    "##### 5. Edge Detection\n",
    "###### Canny Edge Detection (`threshold1`, `threshold2`)\n",
    "- **Parameters**:\n",
    "  - `threshold1`: Lower threshold for weak edges.\n",
    "  - `threshold2`: Upper threshold for strong edges.\n",
    "- **Range**:\n",
    "  - `threshold1` and `threshold2`: `(50, 150)`, `(100, 200)`, `(150, 250)`, `(200, 300)`\n",
    "  - Lower values detect more edges, useful for detailed images; higher values highlight stronger, more defined edges.\n",
    "\n",
    "###### Sobel Edge Detection (`ksize`, `scale`, `delta`, `borderType`)\n",
    "- **Parameters**:\n",
    "  - `ksize`: Kernel size for the Sobel operator.\n",
    "  - `scale`: Scaling factor for gradients.\n",
    "  - `delta`: Value added to the result.\n",
    "  - `borderType`: Border handling for edges.\n",
    "- **Range**:\n",
    "  - `ksize`: `3`, `5`, `7`\n",
    "  - `scale`: `1`, `2`\n",
    "  - `delta`: `0` (default)\n",
    "  - `borderType`: `cv2.BORDER_DEFAULT`\n",
    "  - Adjusts the level of detail and sharpness captured by the filter.\n",
    "\n",
    "###### Unsharp Masking (`amount`, `kernel_size`)\n",
    "- **Parameters**:\n",
    "  - `amount`: Strength of sharpening effect applied to the image.\n",
    "  - `kernel_size`: Size of the kernel used for blurring in the unsharp mask process.\n",
    "- **Range**:\n",
    "  - `amount`: `1.0` to `2.5` (Higher values produce stronger sharpening)\n",
    "  - `kernel_size`: `(3, 3)`, `(5, 5)`, `(7, 7)`, `(9, 9)`\n",
    "  - Sharpening enhances edges and contrasts to make features more prominent, but excessively high values can introduce artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa67a991c4e1ef8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define expanded parameter grids for hyperparameter tuning for each technique\n",
    "technique_param_grids = {\n",
    "    # Noise Reduction Techniques Parameters\n",
    "    # Gaussian Blur - kernel size affects the degree of blurring\n",
    "    \"Gaussian Blur\": [\n",
    "        {\"ksize\": (3, 3)},  # Small blur, preserves more details while reducing minor noise\n",
    "        {\"ksize\": (5, 5)},  # Moderate blur, balances noise reduction and detail preservation\n",
    "        {\"ksize\": (7, 7)},  # Stronger blur, reduces more noise but may lose more details\n",
    "        {\"ksize\": (9, 9)}   # High blur, significant reduction of noise, more detail loss\n",
    "    ],\n",
    "\n",
    "    # Median Blur - kernel size affects the reduction of salt-and-pepper noise\n",
    "    \"Median Blur\": [\n",
    "        {\"ksize\": 3},  # Small kernel, effective for minor salt-and-pepper noise\n",
    "        {\"ksize\": 5},  # Moderate kernel, more aggressive noise reduction\n",
    "        {\"ksize\": 7},  # Large kernel, used for significant salt-and-pepper noise reduction\n",
    "        {\"ksize\": 9}   # Largest kernel, aggressive noise reduction but may lose finer details\n",
    "    ],\n",
    "\n",
    "    # Non-Local Means - affects noise reduction strength and quality\n",
    "    \"Non-Local Means\": [\n",
    "        {\"h\": 5, \"templateWindowSize\": 7, \"searchWindowSize\": 21},   # Low filter strength (h), smaller template\n",
    "        {\"h\": 10, \"templateWindowSize\": 7, \"searchWindowSize\": 21},  # Moderate filter strength (h), balance of denoising and details\n",
    "        {\"h\": 15, \"templateWindowSize\": 7, \"searchWindowSize\": 21},  # Strong filter strength, more noise reduction but risk of over-smoothing\n",
    "        {\"h\": 20, \"templateWindowSize\": 10, \"searchWindowSize\": 31}  # Higher strength and larger search windows for stronger denoising\n",
    "    ],\n",
    "\n",
    "    # Histogram Equalization Techniques Parameters\n",
    "    # CLAHE - clip limit controls contrast, tile grid size controls local regions\n",
    "    \"CLAHE\": [\n",
    "        {\"clipLimit\": 2.0, \"tileGridSize\": (8, 8)},  # Low clip limit, preserves global contrast, effective for mild contrast enhancement\n",
    "        {\"clipLimit\": 3.0, \"tileGridSize\": (8, 8)},  # Moderate clip limit, better enhancement for darker/lighter regions\n",
    "        {\"clipLimit\": 4.0, \"tileGridSize\": (4, 4)},  # Higher clip limit, can lead to artifacts but increases local contrast\n",
    "        {\"clipLimit\": 5.0, \"tileGridSize\": (6, 6)}   # High clip limit, strong local contrast enhancement\n",
    "    ],\n",
    "\n",
    "    # Binarization Techniques Parameters\n",
    "    # Global Threshold - value for the threshold, used to separate foreground from background\n",
    "    \"Global Threshold\": [\n",
    "        {\"thresholdValue\": 100},  # Low threshold, makes more areas white, may overexpose\n",
    "        {\"thresholdValue\": 127},  # Middle threshold, balance between foreground and background\n",
    "        {\"thresholdValue\": 150},  # High threshold, less white, more black areas\n",
    "        {\"thresholdValue\": 200}   # Higher threshold, darkest parts retained as foreground\n",
    "    ],\n",
    "\n",
    "    # Adaptive Threshold - block size and constant C, used for adaptive thresholding\n",
    "    \"Adaptive Threshold\": [\n",
    "        {\"adaptiveMethod\": cv2.ADAPTIVE_THRESH_MEAN_C, \"blockSize\": 11, \"C\": 2},  # Small block size, captures smaller variations\n",
    "        {\"adaptiveMethod\": cv2.ADAPTIVE_THRESH_MEAN_C, \"blockSize\": 15, \"C\": 3},  # Larger block size, averages larger areas\n",
    "        {\"adaptiveMethod\": cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \"blockSize\": 11, \"C\": 2},  # Gaussian weighting, better for uneven lighting\n",
    "        {\"adaptiveMethod\": cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \"blockSize\": 15, \"C\": 3}   # Larger area, smoother output\n",
    "    ],\n",
    "\n",
    "    \"Otsu Threshold\": [\n",
    "        {}  # No parameters needed, automatic threshold selection\n",
    "    ],\n",
    "    \n",
    "    \"Inverted Otsu Threshold\": [{}],\n",
    "\n",
    "    # Morphological Operations Techniques Parameters\n",
    "    # Dilation - kernel size affects how much an object is expanded, helps to highlight and connect features in the image\n",
    "    \"Dilation\": [\n",
    "        {\"kernel_size\": (3, 3)},  # Small kernel, slight expansion of features\n",
    "        {\"kernel_size\": (5, 5)},  # Medium kernel, moderate expansion, often used to fill small holes\n",
    "        {\"kernel_size\": (7, 7)},  # Larger kernel, more significant expansion, fills larger gaps\n",
    "        {\"kernel_size\": (9, 9)}   # Largest kernel, aggressive expansion, can connect disjoint parts\n",
    "    ],\n",
    "\n",
    "    # Erosion - kernel size affects how much an object is eroded, used to reduce noise by shrinking foreground areas\n",
    "    \"Erosion\": [\n",
    "        {\"kernel_size\": (3, 3)},  # Small kernel, minimal shrinking of features\n",
    "        {\"kernel_size\": (5, 5)},  # Medium kernel, reduces small noise while keeping the main features intact\n",
    "        {\"kernel_size\": (7, 7)},  # Larger kernel, removes more fine details, useful for stronger noise reduction\n",
    "        {\"kernel_size\": (9, 9)}   # Largest kernel, aggressive erosion, may result in significant information loss\n",
    "    ],\n",
    "\n",
    "    # Morphological Opening - kernel size affects noise removal, used for removing small white noise from black backgrounds\n",
    "    \"Morphological Opening\": [\n",
    "        {\"kernel_size\": (3, 3)},  # Small kernel, removes small white noise but keeps the main structure\n",
    "        {\"kernel_size\": (5, 5)},  # Medium kernel, better noise removal, may affect finer details\n",
    "        {\"kernel_size\": (7, 7)}   # Larger kernel, stronger noise reduction, potentially removes small features\n",
    "    ],\n",
    "\n",
    "    # Morphological Closing - kernel size affects how gaps in foreground objects are filled, used to close small black holes within objects\n",
    "    \"Morphological Closing\": [\n",
    "        {\"kernel_size\": (3, 3)},  # Small kernel, fills tiny holes, maintains object shape\n",
    "        {\"kernel_size\": (5, 5)},  # Medium kernel, closes medium-sized gaps, useful for refining object borders\n",
    "        {\"kernel_size\": (7, 7)}   # Larger kernel, aggressively closes gaps, useful for solidifying larger structures\n",
    "    ],\n",
    "\n",
    "    # Edge Detection Techniques Parameters\n",
    "    # Canny Edge Detection - lower and upper thresholds for edge linking\n",
    "    \"Canny Edge\": [\n",
    "        {\"threshold1\": 50, \"threshold2\": 150},  # Low thresholds, more edges detected\n",
    "        {\"threshold1\": 100, \"threshold2\": 200},  # Moderate thresholds, balanced edge detection\n",
    "        {\"threshold1\": 150, \"threshold2\": 250},  # High thresholds, only strong edges detected\n",
    "        {\"threshold1\": 200, \"threshold2\": 300}   # Very high thresholds, detects fewer edges, focused on major features\n",
    "    ],\n",
    "\n",
    "    # Sobel Edge - kernel size, scale, delta, border type for Sobel edge detection\n",
    "    \"Sobel Edge\": [\n",
    "        {\"ksize\": 3, \"scale\": 1, \"delta\": 0, \"borderType\": cv2.BORDER_DEFAULT},  # Small kernel, detects finer details\n",
    "        {\"ksize\": 5, \"scale\": 1, \"delta\": 0, \"borderType\": cv2.BORDER_DEFAULT},  # Medium kernel, balances detail and noise suppression\n",
    "        {\"ksize\": 7, \"scale\": 1, \"delta\": 0, \"borderType\": cv2.BORDER_DEFAULT},  # Larger kernel, captures broader gradients\n",
    "        {\"ksize\": 3, \"scale\": 2, \"delta\": 0, \"borderType\": cv2.BORDER_DEFAULT}   # Increased scale, emphasizes detected gradients more strongly\n",
    "    ],\n",
    "    \"Unsharp Masking\": [\n",
    "        {\"amount\": 1.0, \"kernel_size\": (3, 3)},\n",
    "        {\"amount\": 1.5, \"kernel_size\": (5, 5)},\n",
    "        {\"amount\": 2.0, \"kernel_size\": (7, 7)},\n",
    "        {\"amount\": 2.5, \"kernel_size\": (9, 9)}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a00a06b7381b7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run hyperparameter tuning\n",
    "tuned_results = hyperparameter_tuning(total_images, best_techniques_list, technique_param_grids, advanced_evaluation)\n",
    "print(\"\\nTuned Results:\")\n",
    "\n",
    "for step_name, result in tuned_results.items():\n",
    "    print(f\"{step_name}: Best Parameters: {result['Best Parameters']}, Best Score: {result['Best Score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb737e3b48923f3b",
   "metadata": {},
   "source": [
    "#### Interpretations of Results\n",
    "\n",
    "Let's break down and interpret the hyperparameter tuning results for each step. Here's what each section tells us:\n",
    "\n",
    "##### **1. Noise Reduction: Median Blur**\n",
    "- **Best Parameters**: `{'ksize': 3}`\n",
    "- **Best Score**: `-0.03`\n",
    "\n",
    "**Interpretation**:\n",
    "- **Median Blur** was selected as the best noise reduction technique.\n",
    "- The slightly negative score (`-0.03`) indicates a small deviation from the original characteristics, with `ksize=3` performing the best among the tested values. This result shows that a smaller kernel size effectively reduces noise while avoiding excessive smoothing, which helps retain image clarity. Since noise negatively impacts image quality, preserving details while reducing noise was key to achieving a good balance.\n",
    "\n",
    "##### **2. Histogram Equalization: Histogram Equalization**\n",
    "- **Best Parameters**: `None`\n",
    "- **Best Score**: `-inf`\n",
    "\n",
    "**Interpretation**:\n",
    "- The hyperparameter tuning for **Histogram Equalization** did not find any beneficial parameters, resulting in a score of `-inf`. This suggests that histogram equalization might not be effective for enhancing this specific dataset. Given that histogram equalization aims to enhance contrast, the lack of improvement implies that the initial contrast levels may already have been optimal, or that equalization introduced inconsistencies that were detrimental to the image quality.\n",
    "\n",
    "##### **3. Binarization: Adaptive Threshold**\n",
    "- **Best Parameters**: `{'adaptiveMethod': 1, 'blockSize': 11, 'C': 2}`\n",
    "- **Best Score**: `78.84`\n",
    "\n",
    "**Interpretation**:\n",
    "- The **Adaptive Threshold** method, using the **Gaussian adaptive method** (`adaptiveMethod=1`), a **block size** of `11`, and a **constant C** of `2`, achieved a high score of `78.84`.\n",
    "- This positive score indicates effective enhancement of key metrics such as contrast and sharpness, as well as noise reduction. The use of a **Gaussian adaptive method** along with a smaller block size allowed for better handling of local variations in illumination, which improved segmentation and detail clarity. The evaluation rewarded the technique because contrast was notably improved, which is crucial for extracting meaningful features in the images.\n",
    "\n",
    "##### **4. Morphological Operations: Dilation**\n",
    "- **Best Parameters**: `{'kernel_size': (9, 9)}`\n",
    "- **Best Score**: `-0.01`\n",
    "\n",
    "**Interpretation**:\n",
    "- **Dilation** was selected as the best morphological operation for this dataset.\n",
    "- A **kernel size of `(9, 9)`** provided the least negative score (`-0.01`), indicating that the larger kernel size was successful in connecting fragmented elements, which improved the structure of features in the image. The score's slight negativity implies a trade-off, where some minor details were lost, but the benefit of connecting important features outweighed the drawbacks, especially in terms of preparing the images for content recognition.\n",
    "\n",
    "##### **5. Edge Detection: Canny Edge**\n",
    "- **Best Parameters**: `{'threshold1': 50, 'threshold2': 150}`\n",
    "- **Best Score**: `12.23`\n",
    "\n",
    "**Interpretation**:\n",
    "- The **Canny Edge Detection** method, with **thresholds of `50` and `150`**, achieved a score of `12.23`.\n",
    "- The positive score indicates that these threshold values effectively highlighted edges, which was beneficial for extracting structures such as table lines and text outlines. The evaluation rewarded the improvement in contrast and sharpness, which contributed to making the details clearer and reducing the impact of noise. Lower thresholds allowed the detection of more edges, which in turn enhanced the structure and features of the image.\n",
    "\n",
    "##### **Summary & Key Insights**:\n",
    "\n",
    "1. **Negative Scores**:\n",
    "   - Negative scores (e.g., **Noise Reduction** and **Morphological Operations**) indicate that the processed images deviated slightly from the original characteristics in ways that were detrimental based on the evaluation metrics.\n",
    "   - The least negative scores represent the best parameters that resulted in minimal detrimental changes while preserving key features.\n",
    "\n",
    "2. **Positive Scores**:\n",
    "   - Positive scores (e.g., **Binarization** and **Edge Detection**) indicate significant improvements in metrics such as **contrast**, **sharpness**, and **detection of elements**.\n",
    "   - Techniques with higher positive scores effectively enhanced image quality by improving features that are crucial for content recognition and extraction.\n",
    "\n",
    "3. **Histogram Equalization Issue**:\n",
    "   - The score of `-inf` for **Histogram Equalization** indicates that this technique was not effective for improving the evaluated characteristics. This could imply that the dataset's initial contrast was already optimal, or that the equalization process introduced artifacts that degraded the overall quality.\n",
    "\n",
    "4. **Best Techniques Overview**:\n",
    "   - For each step, the parameter combination with the highest score (least negative or most positive) was selected.\n",
    "   - It is important to note that the magnitude of scores can vary greatly, depending on the evaluation metrics and their relative weights. In this evaluation, **contrast** and **sharpness** were given higher importance, leading to higher scores for techniques that excelled in these areas.\n",
    "\n",
    "##### Recommendations:\n",
    "- **Median Blur (Noise Reduction)**: A smaller kernel size (`ksize=3`) provided the least negative impact, suggesting that moderate noise reduction without excessive smoothing was most effective.\n",
    "- **Adaptive Threshold (Binarization)**: Using a **Gaussian adaptive method** with a **smaller block size** resulted in the highest improvement in contrast and sharpness, which are crucial for effective segmentation.\n",
    "- **Dilation (Morphological Operations)**: A larger kernel size (`9x9`) helped enhance the structure of the features, which was beneficial for preparing the images for subsequent recognition steps.\n",
    "- **Canny Edge Detection**: Lower thresholds (`50` and `150`) proved effective for enhancing edge details, resulting in a high score and indicating that comprehensive edge detection was valuable for downstream tasks.\n",
    "\n",
    "Overall, the results are consistent with expectations—lower parameters (e.g., less aggressive filtering or morphological operations) often preserve original characteristics better, while edge detection benefits from more comprehensive edge capturing with lower thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f0ec865424bf5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
