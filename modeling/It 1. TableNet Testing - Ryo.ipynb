{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1 - TableNet Model Testing\n",
    "\n",
    "> **Model**: TableNet Model <br/>\n",
    "> **Creator**: Ryo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tkinter import Tk, filedialog\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Specify the Tesseract command location for Windows\n",
    "pytesseract.pytesseract.tesseract_cmd = r'D:\\Software\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_file(model_path):\n",
    "    \"\"\"\n",
    "    Load the deep learning model for table detection.\n",
    "\n",
    "    Parameters:\n",
    "        model_path (str): Path to the .h5 file of the model.\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded model for prediction.\n",
    "    \"\"\"\n",
    "    # Step 1: Load the model from the specified path\n",
    "    model = load_model(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'tablenet_model.h5'\n",
    "model = load_model_from_file(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_image_file():\n",
    "    \"\"\"\n",
    "    Open a file dialog to select an image file for local testing.\n",
    "\n",
    "    Returns:\n",
    "        str: Path of the selected image file.\n",
    "    \"\"\"\n",
    "    # Step 1: Open a file dialog to select the image\n",
    "    Tk().withdraw()  # Hide the root window\n",
    "    image_path = filedialog.askopenfilename(\n",
    "        title=\"Select an Image File\", \n",
    "        filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff *.tif\")]\n",
    "    )\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = select_image_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, image_size=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Load and preprocess the image for model prediction.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path of the selected image file.\n",
    "        image_size (tuple): Size to which the image is resized for the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The resized and original images.\n",
    "    \"\"\"\n",
    "    # Step 1: Read the image from the path\n",
    "    original_image = cv2.imread(image_path)\n",
    "    \n",
    "    # Step 2: Convert the image from BGR to RGB\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Step 3: Resize the image to the target size\n",
    "    resized_image = cv2.resize(original_image, image_size)\n",
    "    \n",
    "    return resized_image, original_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_image, original_image = load_and_preprocess_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(model, resized_image, original_image):\n",
    "    \"\"\"\n",
    "    Run prediction on the image and visualize the original, table mask, and column mask.\n",
    "\n",
    "    Parameters:\n",
    "        model: Loaded model for table detection.\n",
    "        resized_image: Preprocessed image to be input into the model.\n",
    "        original_image: Original image before preprocessing.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Table mask from the model prediction.\n",
    "    \"\"\"\n",
    "    # Step 1: Run the prediction using the model\n",
    "    predictions = model.predict(resized_image[np.newaxis, ...] / 255.0)\n",
    "    \n",
    "    # Step 2: Extract the table and column masks\n",
    "    table_mask = predictions[1][0, ..., 0]\n",
    "    column_mask = predictions[0][0, ..., 0]\n",
    "    \n",
    "    # Step 3: Visualize the original image, table mask, and column mask side by side\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    # Original Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_image)\n",
    "    plt.title('Input Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Table Mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(table_mask, cmap='gray')\n",
    "    plt.title('Predicted Table Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Column Mask\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(column_mask, cmap='gray')\n",
    "    plt.title('Predicted Column Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return table_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_mask = predict_and_visualize(model, resized_image, original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Table Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_table_areas(mask, image, threshold=0.1, min_area=500):\n",
    "    \"\"\"\n",
    "    Efficiently detect table areas from the mask using contours and thresholding.\n",
    "\n",
    "    Parameters:\n",
    "        mask (np.array): Mask image for table detection.\n",
    "        image (np.array): Original image to match the mask size.\n",
    "        threshold (float): Threshold for binarizing the mask.\n",
    "        min_area (int): Minimum area threshold for contour detection.\n",
    "\n",
    "    Returns:\n",
    "        list: Detected contours above the minimum area.\n",
    "    \"\"\"\n",
    "    # Step 1: Perform thresholding to binarize the mask\n",
    "    binary_mask = (mask > threshold).astype(np.uint8) * 255\n",
    "\n",
    "    # Step 2: Detect contours in the binary mask\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Step 3: Filter only large contours based on minimum area\n",
    "    large_contours = [c for c in contours if cv2.contourArea(c) >= min_area]\n",
    "\n",
    "    return large_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Detected Table Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_table_areas(image, contours):\n",
    "    \"\"\"\n",
    "    Visualize the detected table areas with green bounding boxes using Matplotlib.\n",
    "    It also prints the area of each detected contour on top of the bounding box.\n",
    "\n",
    "    Parameters:\n",
    "        image (np.array): Original image where tables are detected.\n",
    "        contours (list): Contours of detected table areas.\n",
    "    \"\"\"\n",
    "    # Step 1: Early exit if there are no contours (to avoid unnecessary processing)\n",
    "    if not contours:\n",
    "        print(\"No contours detected, skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Copy the original image for visualization\n",
    "    vis_image = image.copy()\n",
    "\n",
    "    # Step 3: Draw green bounding boxes for each detected contour and add area text\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        area = cv2.contourArea(contour)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(vis_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Display the area above the bounding box\n",
    "        area_text = f'Area: {int(area)}'\n",
    "        cv2.putText(vis_image, area_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Step 4: Display the image with bounding boxes and area text\n",
    "    plt.figure(figsize=(36, 24))\n",
    "    plt.imshow(vis_image)\n",
    "    plt.title('Detected Table Areas with Bounding Box and Area Info')\n",
    "    plt.axis('off')  # Hide axis for cleaner visualization\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Table Detection and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table_detection_and_visualization(mask, image, threshold=0.1, min_area=500, visualize=True):\n",
    "    \"\"\"\n",
    "    Efficiently detect and optionally visualize table areas.\n",
    "\n",
    "    Parameters:\n",
    "        mask (np.array): The table mask for detection.\n",
    "        image (np.array): The original image to visualize detected tables.\n",
    "        threshold (float): Threshold for binarizing the mask.\n",
    "        min_area (int): Minimum area threshold for contour detection.\n",
    "        visualize (bool): Whether or not to visualize the detected areas.\n",
    "\n",
    "    Returns:\n",
    "        list: Detected contours above the minimum area.\n",
    "    \"\"\"\n",
    "    # Step 1: Ensure the mask is resized to match the image dimensions\n",
    "    if mask.shape[:2] != image.shape[:2]:\n",
    "        mask = cv2.resize(mask, (image.shape[1], image.shape[0]))  # Resize mask to match image size\n",
    "\n",
    "    # Step 2: Detect table areas\n",
    "    contours = detect_table_areas(mask, image, threshold=threshold, min_area=min_area)\n",
    "\n",
    "    # Step 3: Visualize only if required (avoid unnecessary heavy processing)\n",
    "    if visualize:\n",
    "        visualize_table_areas(image, contours)\n",
    "\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and optionally visualize table areas with optimized flow\n",
    "large_contours = process_table_detection_and_visualization(\n",
    "    mask=table_mask, \n",
    "    image=original_image, \n",
    "    threshold=0.1, \n",
    "    min_area=150000, \n",
    "    visualize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Combined Table Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_combined_table_area(contours, padding=20):\n",
    "    \"\"\"\n",
    "    Combine all detected table contours into one bounding box.\n",
    "\n",
    "    Parameters:\n",
    "        contours (list): Contours of detected table areas.\n",
    "        padding (int): Padding around the combined bounding box.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Combined bounding box (x, y, x2, y2).\n",
    "    \"\"\"\n",
    "    if not contours:\n",
    "        return None\n",
    "    \n",
    "    # Step 1: Initialize the combined bounding box with the first contour\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "    combined_bbox = [x, y, x + w, y + h]\n",
    "    \n",
    "    # Step 2: Combine all bounding boxes\n",
    "    for contour in contours[1:]:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        combined_bbox[0] = min(combined_bbox[0], x)\n",
    "        combined_bbox[1] = min(combined_bbox[1], y)\n",
    "        combined_bbox[2] = max(combined_bbox[2], x + w)\n",
    "        combined_bbox[3] = max(combined_bbox[3], y + h)\n",
    "    \n",
    "    # Step 3: Add padding to the combined bounding box\n",
    "    combined_bbox[0] = max(0, combined_bbox[0] - padding)\n",
    "    combined_bbox[1] = max(0, combined_bbox[1] - padding)\n",
    "    combined_bbox[2] += padding\n",
    "    combined_bbox[3] += padding\n",
    "    \n",
    "    return combined_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_bbox = detect_combined_table_area(large_contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Combined Table Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_combined_table_area(image, combined_bbox):\n",
    "    \"\"\"\n",
    "    Visualize the combined bounding box on the original image using Matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "        image (np.array): Original image.\n",
    "        combined_bbox (tuple): Combined bounding box (x, y, x2, y2).\n",
    "    \"\"\"\n",
    "    # Step 1: Copy the original image for visualization\n",
    "    vis_image = image.copy()\n",
    "\n",
    "    # Step 2: Draw the combined bounding box in green\n",
    "    x, y, x2, y2 = combined_bbox\n",
    "    cv2.rectangle(vis_image, (x, y), (x2, y2), (0, 255, 0), 2)  # Green bounding box\n",
    "    \n",
    "    # Step 3: Display the image with the combined bounding box using Matplotlib\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(vis_image)\n",
    "    plt.title('Combined Table Area')\n",
    "    plt.axis('off')  # Hide axis for cleaner visualization\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Detect the combined bounding box from the contours\n",
    "combined_bbox = detect_combined_table_area(large_contours, padding=20)\n",
    "\n",
    "# Step 2: Visualize the combined bounding box on the original image\n",
    "if combined_bbox:\n",
    "    visualize_combined_table_area(original_image, combined_bbox)\n",
    "else:\n",
    "    print(\"No combined bounding box detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop Table Areas Based on Detected Bounding Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_table_area(image, bbox):\n",
    "    \"\"\"\n",
    "    Crop the table area from the image using the combined bounding box.\n",
    "\n",
    "    Parameters:\n",
    "        image (np.array): Original image where tables are detected.\n",
    "        bbox (tuple): Bounding box coordinates (x, y, x2, y2) for the table area.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Cropped table image.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract the bounding box coordinates\n",
    "    x, y, x2, y2 = bbox\n",
    "    \n",
    "    # Step 2: Crop the image based on the bounding box\n",
    "    cropped_image = image[y:y2, x:x2]\n",
    "    \n",
    "    # Step 3: Visualize the cropped table area\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cropped_image)\n",
    "    plt.title('Cropped Table Area')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if combined_bbox:\n",
    "    cropped_table_image = crop_table_area(original_image, combined_bbox)\n",
    "else:\n",
    "    print(\"No combined table area detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform OCR on the Cropped Table and Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_and_visualize_ocr(image, confidence_threshold=40):\n",
    "    \"\"\"\n",
    "    Perform OCR on the table image and visualize the results with predicted text in bounding boxes.\n",
    "    \n",
    "    Parameters:\n",
    "        image (np.array): Original table image.\n",
    "        confidence_threshold (int): Confidence threshold to filter OCR results.\n",
    "    \n",
    "    Returns:\n",
    "        str: Detected text string.\n",
    "    \"\"\"\n",
    "    # Convert the image to PIL format for Pytesseract\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Define multiple OCR configurations\n",
    "    configs = [\n",
    "        r'--oem 3 --psm 6',\n",
    "        r'--oem 3 --psm 4',\n",
    "        r'--oem 3 --psm 3',\n",
    "        r'--oem 3 --psm 11',\n",
    "        r'--oem 3 --psm 12'\n",
    "    ]\n",
    "    \n",
    "    all_text = []\n",
    "    all_boxes = []\n",
    "    \n",
    "    for config in configs:\n",
    "        custom_config = f\"{config} -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,()-_/%' \"\n",
    "        ocr_result = pytesseract.image_to_data(pil_image, config=custom_config, output_type=pytesseract.Output.DICT)\n",
    "        \n",
    "        for i in range(len(ocr_result['text'])):\n",
    "            if int(ocr_result['conf'][i]) >= confidence_threshold:\n",
    "                text = ocr_result['text'][i]\n",
    "                if text.strip():\n",
    "                    all_text.append(text)\n",
    "                    all_boxes.append((\n",
    "                        ocr_result['left'][i],\n",
    "                        ocr_result['top'][i],\n",
    "                        ocr_result['width'][i],\n",
    "                        ocr_result['height'][i],\n",
    "                        text\n",
    "                    ))\n",
    "    \n",
    "    # Visualize results\n",
    "    vis_image = image.copy()\n",
    "    for (x, y, w, h, text) in all_boxes:\n",
    "        # Draw thin blue bounding box\n",
    "        cv2.rectangle(vis_image, (x, y), (x + w, y + h), (255, 0, 0), 1)  # Blue color, thickness 1\n",
    "        \n",
    "        # Prepare text background\n",
    "        (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.3, 1)\n",
    "        text_offset_x = x\n",
    "        text_offset_y = y - 5\n",
    "        box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height - 2))\n",
    "        cv2.rectangle(vis_image, box_coords[0], box_coords[1], (255, 0, 0), cv2.FILLED)  # Blue background\n",
    "        \n",
    "        # Put text\n",
    "        cv2.putText(vis_image, text, (text_offset_x, text_offset_y), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"OCR Result Visualization\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    detected_text_str = ' '.join(all_text)\n",
    "    print(\"Detected Text:\")\n",
    "    print(detected_text_str)\n",
    "    \n",
    "    return detected_text_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform OCR and visualize the results in one step\n",
    "ocr_result = perform_and_visualize_ocr(cropped_table_image, confidence_threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Test Typed Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    Loads an image from the specified file path, converts it to a bytes-like object,\n",
    "    and returns the image as a NumPy array.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The image in NumPy array format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the image and load it into a BytesIO object\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Optionally convert to array (NumPy)\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        return img_np\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_image_path = \"../data/10.1.1.1.2018_4.jpeg\"\n",
    "ocr_image = load_image(ocr_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform OCR and visualize the results in one step\n",
    "ocr_result = perform_and_visualize_ocr(ocr_image, confidence_threshold=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alessandroryo",
   "language": "python",
   "name": "alessandroryo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
